{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1155b5f7-1723-4f88-9d0b-ae82a8e4ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Saved parsed output to 'tt_subset_em_parsed.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "# Step 1: Load the .txt file\n",
    "with open(\"tt_subset_em.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Step 2: Read it into a DataFrame using pandas\n",
    "df = pd.read_csv(StringIO(raw_text))\n",
    "\n",
    "# Step 3: Parse the `text_final` field which contains embedded JSON\n",
    "def safe_parse_json(json_string):\n",
    "    try:\n",
    "        cleaned = json_string.strip().strip(\"`\").strip(\"json\").strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "df[\"parsed_json\"] = df[\"text_final\"].apply(safe_parse_json)\n",
    "\n",
    "# Step 4: Extract `actions` and `count`\n",
    "df[\"actions\"] = df[\"parsed_json\"].apply(lambda x: x.get(\"actions\", []))\n",
    "df[\"count\"] = df[\"parsed_json\"].apply(lambda x: x.get(\"count\", 0))\n",
    "\n",
    "# Optional: Save to JSON\n",
    "df.to_json(\"tt_subset_em_parsed.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"✅ Done! Saved parsed output to 'tt_subset_em_parsed.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b116a2e-11f1-4ded-8adb-d7ad0c01d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data saved to 'tt_subset_em_expanded.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load your JSON file\n",
    "with open(\"tt_subset_em_parsed.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Step 2: Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Parse 'best_res_perplex'\n",
    "def parse_best_res_perplex(cell):\n",
    "    try:\n",
    "        cleaned = cell.strip().lstrip(\"```json\").rstrip(\"```\").strip()\n",
    "        return json.loads(cleaned)\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "df[\"parsed_best\"] = df[\"best_res_perplex\"].apply(parse_best_res_perplex)\n",
    "\n",
    "# Step 4: Extract fields\n",
    "df[\"actions\"] = df[\"parsed_best\"].apply(lambda x: x.get(\"actions\", []) if isinstance(x, dict) else [])\n",
    "df[\"count\"] = df[\"parsed_best\"].apply(lambda x: x.get(\"count\", 0) if isinstance(x, dict) else 0)\n",
    "\n",
    "# Step 5: Drop unnecessary columns\n",
    "df.drop(columns=[\"best_res_perplex\", \"parsed_best\"], inplace=True)\n",
    "\n",
    "# Step 6: Save the cleaned and expanded data\n",
    "df.to_json(\"tt_subset_em_expanded.json\", orient=\"records\", indent=2)\n",
    "print(\"✅ Cleaned data saved to 'tt_subset_em_expanded.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95f85b3b-81c9-44f3-afdd-1d7699de89e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing Unnamed: 0\n",
      "🔍 Processing Unnamed: 4\n",
      "🔍 Processing Unnamed: 6\n",
      "🔍 Processing Unnamed: 8\n",
      "🔍 Processing Unnamed: 12\n",
      "🔍 Processing Unnamed: 13\n",
      "🔍 Processing Unnamed: 15\n",
      "🔍 Processing Unnamed: 16\n",
      "🔍 Processing Unnamed: 17\n",
      "🔍 Processing Unnamed: 20\n",
      "🔍 Processing Unnamed: 22\n",
      "🔍 Processing Unnamed: 23\n",
      "🔍 Processing Unnamed: 25\n",
      "🔍 Processing Unnamed: 26\n",
      "🔍 Processing Unnamed: 27\n",
      "🔍 Processing Unnamed: 28\n",
      "🔍 Processing Unnamed: 29\n",
      "🔍 Processing Unnamed: 30\n",
      "🔍 Processing Unnamed: 31\n",
      "🔍 Processing Unnamed: 33\n",
      "🔍 Processing Unnamed: 34\n",
      "🔍 Processing Unnamed: 35\n",
      "✅ Done. Saved annotated file to: all_entries_annotated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load JSON\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Prompt generator for each entry\n",
    "def generate_gpt_prompt_from_json(entry):\n",
    "    text_final = entry[\"text_final\"]\n",
    "    actions = entry[\"actions\"]\n",
    "    paragraphs = text_final.split(\"||\")\n",
    "\n",
    "    actions_section = \"### Actions:\\n\"\n",
    "    for action in actions:\n",
    "        actions_section += f\"Action ID: {action['id']}\\nSummary: {action['summary']}\\n\\n\"\n",
    "\n",
    "    paragraphs_section = \"### Text (paragraphs are separated by ||):\\n\"\n",
    "    for i, para in enumerate(paragraphs):\n",
    "        clean_para = para.strip().replace(\"\\n\", \" \")\n",
    "        paragraphs_section += f\"Para {i}: {clean_para}\\n\"\n",
    "\n",
    "    final_prompt = f\"\"\"### Task:\n",
    "You are a social media design and feature expert analyzing a platform’s news release. Your goal is to identify and group all **distinct** digital well-being changes and map them to **all supporting paragraphs**, including those that may **loosely or indirectly** refer to each change.\n",
    "\n",
    "### Your Process:\n",
    "1. Read all paragraphs carefully, even if they appear generic or promotional.\n",
    "2. Read the actions and summaries of all the actions.\n",
    "3. Group together all paragraphs that refer to the same change — even indirectly.\n",
    "   - Include introductory, descriptive, technical, policy-related, or promotional language as supporting evidence.\n",
    "   - Be generous in mapping — include all paragraphs that help understand the change or its purpose.\n",
    "4. Provide an uncertainty score (0–1) indicating your confidence in the classification.\n",
    "\n",
    "{actions_section}\n",
    "{paragraphs_section}\n",
    "\n",
    "### Output Format (for each action):\n",
    "- Action ID: [e.g., A1]\n",
    "- Paragraphs: [list of paragraph numbers or excerpts]\n",
    "- Uncertainty Score: [0–1]\n",
    "\"\"\"\n",
    "    return final_prompt, paragraphs\n",
    "\n",
    "# Call GPT\n",
    "def call_openai_gpt(prompt, client, model=\"gpt-4\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Parse GPT output robustly\n",
    "import re\n",
    "def parse_gpt_output(gpt_output):\n",
    "    action_blocks = re.split(r\"- Action ID:\\s*\", gpt_output)\n",
    "    results = {}\n",
    "    for block in action_blocks[1:]:\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        action_id = lines[0].strip()\n",
    "        para_line = next((line for line in lines if \"paragraph\" in line.lower()), \"\")\n",
    "        para_matches = re.findall(r\"\\d+\", para_line)\n",
    "        para_ids = [int(p) for p in para_matches]\n",
    "        results[action_id] = para_ids\n",
    "    return results\n",
    "\n",
    "# Top-7 similarity\n",
    "def get_top_7_similar_paragraph_texts(paragraphs, summary, client, model=\"text-embedding-3-small\"):\n",
    "    summary_embedding = client.embeddings.create(\n",
    "        input=[summary],\n",
    "        model=model\n",
    "    ).data[0].embedding\n",
    "\n",
    "    paragraph_embeddings = client.embeddings.create(\n",
    "        input=paragraphs,\n",
    "        model=model\n",
    "    ).data\n",
    "\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    similarities = [\n",
    "        cosine_similarity(summary_embedding, para_emb.embedding)\n",
    "        for para_emb in paragraph_embeddings\n",
    "    ]\n",
    "\n",
    "    top_7_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:7]\n",
    "    top_7_paragraphs = [f\"Para {i}: {paragraphs[i].strip()}\" for i in top_7_indices]\n",
    "\n",
    "    return top_7_paragraphs\n",
    "\n",
    "# Update one entry\n",
    "def annotate_entry(entry, client):\n",
    "    gpt_prompt, paragraphs = generate_gpt_prompt_from_json(entry)\n",
    "    gpt_output = call_openai_gpt(gpt_prompt, client)\n",
    "    para_matches = parse_gpt_output(gpt_output)\n",
    "\n",
    "    for action in entry[\"actions\"]:\n",
    "        action_id = action[\"id\"]\n",
    "        summary = action[\"summary\"]\n",
    "        action[\"matched_paragraphs\"] = para_matches.get(action_id, [])\n",
    "        action[\"top_7_similar_paragraphs\"] = get_top_7_similar_paragraph_texts(paragraphs, summary, client)\n",
    "    return entry\n",
    "\n",
    "# === Run full pipeline ===\n",
    "json_path = \"tt_subset_em_expanded.json\"\n",
    "output_path = \"all_entries_annotated.json\"\n",
    "api_key = \"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\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Load and annotate all entries with actions\n",
    "all_data = load_json(json_path)\n",
    "annotated_data = []\n",
    "\n",
    "for entry in all_data:\n",
    "    if entry.get(\"actions\"):\n",
    "        print(f\"🔍 Processing Unnamed: {entry['Unnamed: 0']}\")\n",
    "        try:\n",
    "            updated_entry = annotate_entry(entry, client)\n",
    "            annotated_data.append(updated_entry)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed on Unnamed: {entry['Unnamed: 0']}: {e}\")\n",
    "            annotated_data.append(entry)  # Save original in case of failure\n",
    "    else:\n",
    "        annotated_data.append(entry)\n",
    "\n",
    "# Save everything\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(annotated_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Done. Saved annotated file to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b00010-700c-4459-a94a-976ee9fe3654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
